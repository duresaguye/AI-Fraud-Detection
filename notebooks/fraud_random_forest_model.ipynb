{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9In6hmXT6bof"
      },
      "source": [
        "Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQQDLAfJ442t",
        "outputId": "771b90b0-8d4c-48aa-866e-ffd975d5c60d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.20.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting mlflow-skinny==2.20.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.20.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (17.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.37)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.20.1->mlflow)\n",
            "  Downloading databricks_sdk-0.43.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (1.16.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (4.25.6)\n",
            "Requirement already satisfied: pydantic<3,>=1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (2.10.6)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.1->mlflow) (4.12.2)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (2.27.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.1->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (75.1.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (0.37b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.0->mlflow-skinny==2.20.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.0->mlflow-skinny==2.20.1->mlflow) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.1->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.1->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.1->mlflow) (2025.1.31)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.20.1-py3-none-any.whl (28.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.3/28.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.20.1-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.43.0-py3-none-any.whl (647 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.4/647.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, gunicorn, graphql-core, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 databricks-sdk-0.43.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.20.1 mlflow-skinny-2.20.1\n"
          ]
        }
      ],
      "source": [
        "# Data Handling & Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install mlflow\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Deep Learning Models (Neural Networks)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# MLOps (Experiment Tracking)\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZrqrlfA7voH"
      },
      "source": [
        "load Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DaSRDq879ds",
        "outputId": "1f8b094a-e593-4e53-8845-c4ccb82c0cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   user_id          signup_time        purchase_time  purchase_value  \\\n",
            "0    22058  2015-02-24 22:55:49  2015-04-18 02:47:11       -0.160204   \n",
            "1   333320  2015-06-07 20:39:50  2015-06-08 01:38:54       -1.142592   \n",
            "2     1359  2015-01-01 18:52:44  2015-01-01 18:52:45       -1.197169   \n",
            "3   150084  2015-04-28 21:13:25  2015-05-04 13:54:50        0.385567   \n",
            "4   221365  2015-07-21 07:09:52  2015-09-09 18:40:53        0.112681   \n",
            "\n",
            "       device_id  source  browser  sex  age       ip_address  class  \\\n",
            "0  QVPSPJUOCKZAR       2        0    1   39   73275836879972      0   \n",
            "1  EOGFQPIZPYXFZ       0        0    0   53  350311387865908      0   \n",
            "2  YSSKYOSJHPPLJ       2        3    1   53  262147382011095      1   \n",
            "3  ATGTXKYKUDUQN       2        4    1   41  384054244391396      0   \n",
            "4  NAUITBZFJKHWW       0        4    1   45  415583117452712      0   \n",
            "\n",
            "   transaction_frequency  transaction_velocity  hour_of_day  day_of_week  \n",
            "0                    0.0                   0.0            2            5  \n",
            "1                    0.0                   0.0            1            0  \n",
            "2                    0.0                   0.0           18            3  \n",
            "3                    0.0                   0.0           13            0  \n",
            "4                    0.0                   0.0           18            2  \n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28    Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  0.244200    0.0  \n",
            "1  0.125895 -0.008983  0.014724 -0.342584    0.0  \n",
            "2 -0.139097 -0.055353 -0.059752  1.158900    0.0  \n",
            "3 -0.221929  0.062723  0.061458  0.139886    0.0  \n",
            "4  0.502292  0.219422  0.215153 -0.073813    0.0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Load your preprocessed datasets (ensure the path is correct)\n",
        "fraud_data = pd.read_csv('/content/Preprocessed_Fraud_Data.csv')\n",
        "creditcard_data = pd.read_csv('/content/Preprocessed_Creditcard_Data.csv')\n",
        "\n",
        "# Optional: Check the first few rows\n",
        "print(fraud_data.head())\n",
        "print(creditcard_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU-3KKFH88zf"
      },
      "source": [
        " Feature and Target Separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X44M2YvJ8nKb"
      },
      "outputs": [],
      "source": [
        "# For Fraud Data\n",
        "X_fraud = fraud_data.drop(columns=['class'])\n",
        "y_fraud = fraud_data['class']\n",
        "\n",
        "# For Credit Card Data\n",
        "X_creditcard = creditcard_data.drop(columns=['Class'])\n",
        "y_creditcard = creditcard_data['Class']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5fLpsaB9EaX"
      },
      "source": [
        "Train-Test Split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_sAntYXN9GHy"
      },
      "outputs": [],
      "source": [
        "# For Fraud Data\n",
        "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
        "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33LV4w59QYh"
      },
      "source": [
        " Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVEsskEg97bx",
        "outputId": "9e7ced79-6059-44e9-d8c7-653cb4adb590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 30223 entries, 79867 to 140297\n",
            "Data columns (total 14 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   user_id                30223 non-null  int64  \n",
            " 1   signup_time            30223 non-null  object \n",
            " 2   purchase_time          30223 non-null  object \n",
            " 3   purchase_value         30223 non-null  float64\n",
            " 4   device_id              30223 non-null  object \n",
            " 5   source                 30223 non-null  int64  \n",
            " 6   browser                30223 non-null  int64  \n",
            " 7   sex                    30223 non-null  int64  \n",
            " 8   age                    30223 non-null  int64  \n",
            " 9   ip_address             30223 non-null  int64  \n",
            " 10  transaction_frequency  30223 non-null  float64\n",
            " 11  transaction_velocity   30223 non-null  float64\n",
            " 12  hour_of_day            30223 non-null  int64  \n",
            " 13  day_of_week            30223 non-null  int64  \n",
            "dtypes: float64(3), int64(8), object(3)\n",
            "memory usage: 3.5+ MB\n"
          ]
        }
      ],
      "source": [
        "X_test_fraud.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sXIUu6ev9Rp0"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering on Train/Test Splits (For Fraud Data)\n",
        "for df in [X_train_fraud, X_test_fraud]:\n",
        "    for col in ['signup_time', 'purchase_time']:\n",
        "        df[col] = pd.to_datetime(df[col])\n",
        "        df[col + '_year'] = df[col].dt.year\n",
        "        df[col + '_month'] = df[col].dt.month\n",
        "        df[col + '_day'] = df[col].dt.day\n",
        "        df[col + '_hour'] = df[col].dt.hour\n",
        "\n",
        "# Drop Original Datetime and Categorical Columns (For Fraud Data)\n",
        "for df in [X_train_fraud, X_test_fraud]:\n",
        "    df.drop(columns=['signup_time', 'purchase_time', 'device_id'], inplace=True)\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply scaling\n",
        "X_train_fraud_scaled = scaler.fit_transform(X_train_fraud)\n",
        "X_test_fraud_scaled = scaler.transform(X_test_fraud)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GI8ZfgHBkGv"
      },
      "source": [
        "Model Selection and Training for for Y dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-AinWgGB2I6"
      },
      "source": [
        "Logistic Regression (Baseline Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKB_ZW1sB3Fu",
        "outputId": "a0781ea1-ac2b-4e52-ca50-e39044cc6ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Performance fraud data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95     27393\n",
            "           1       0.00      0.00      0.00      2830\n",
            "\n",
            "    accuracy                           0.91     30223\n",
            "   macro avg       0.45      0.50      0.48     30223\n",
            "weighted avg       0.82      0.91      0.86     30223\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression requires scaled data\n",
        "X_fraud = fraud_data.drop(columns=['class', 'signup_time', 'purchase_time', 'device_id'])\n",
        "y_fraud = fraud_data['class']\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_fraud, y_train_fraud,)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_pred_lr = log_reg.predict(X_test_fraud)  # Predictions on test set\n",
        "print(\"Logistic Regression Performance fraud data:\")\n",
        "print(classification_report(y_test_fraud, y_pred_lr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hBACxEeB-Ho"
      },
      "source": [
        "Decision Tree Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBB1ZyJ6B-_M",
        "outputId": "b01e2270-7e75-4529-9971-32de797bf515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Performance (Credit Card Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.95     27393\n",
            "           1       0.49      0.56      0.52      2830\n",
            "\n",
            "    accuracy                           0.90     30223\n",
            "   macro avg       0.72      0.75      0.73     30223\n",
            "weighted avg       0.91      0.90      0.91     30223\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train_fraud,y_train_fraud,)\n",
        "\n",
        "y_pred_lr = dt_model.predict(X_test_fraud)\n",
        "print(\"Decision Tree Performance (Credit Card Data):\")\n",
        "print(classification_report(y_test_fraud, y_pred_lr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-xRt2whCNSK"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL0QdSW7Cjzf",
        "outputId": "fc260fec-5704-417b-8d27-0250ed2fffbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Performance (Credit Card Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98     27393\n",
            "           1       0.98      0.53      0.69      2830\n",
            "\n",
            "    accuracy                           0.95     30223\n",
            "   macro avg       0.97      0.76      0.83     30223\n",
            "weighted avg       0.96      0.95      0.95     30223\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_fraud, y_train_fraud)\n",
        "\n",
        "y_pred_lr = rf_model.predict(X_test_fraud)\n",
        "print(\"Random Forest Performance (Credit Card Data):\")\n",
        "print(classification_report(y_test_fraud, y_pred_lr ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iy5sQOuCpsl"
      },
      "source": [
        "Gradient Boosting with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY36hZaBCrBU",
        "outputId": "e8971fd0-b3be-40f8-ce15-545fc2e7705c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "XGBoost Performance (Credit Card Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98     27393\n",
            "           1       0.98      0.53      0.69      2830\n",
            "\n",
            "    accuracy                           0.95     30223\n",
            "   macro avg       0.97      0.76      0.83     30223\n",
            "weighted avg       0.96      0.95      0.95     30223\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train_fraud, y_train_fraud)\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_test_fraud)\n",
        "print(\"XGBoost Performance (Credit Card Data):\")\n",
        "print(classification_report(y_test_fraud, y_pred_xgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_bRiZCcHH2b"
      },
      "source": [
        "Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTjR4rGtHI5w"
      },
      "source": [
        "Multi-Layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0bSLOGsIksN"
      },
      "source": [
        "An MLP is a basic feed-forward neural network suitable for tabular data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh2-bvnzHQDl",
        "outputId": "eda2547b-3bf2-4a4a-d510-e8e8f8686dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9267 - loss: 0.2401 - val_accuracy: 0.9541 - val_loss: 0.1875\n",
            "Epoch 2/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.9562 - loss: 0.1794 - val_accuracy: 0.9550 - val_loss: 0.1832\n",
            "Epoch 3/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - accuracy: 0.9568 - loss: 0.1767 - val_accuracy: 0.9549 - val_loss: 0.1822\n",
            "Epoch 4/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.9568 - loss: 0.1763 - val_accuracy: 0.9549 - val_loss: 0.1825\n",
            "Epoch 5/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.9559 - loss: 0.1789 - val_accuracy: 0.9550 - val_loss: 0.1825\n",
            "Epoch 6/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - accuracy: 0.9561 - loss: 0.1784 - val_accuracy: 0.9551 - val_loss: 0.1831\n",
            "Epoch 7/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1784 - val_accuracy: 0.9552 - val_loss: 0.1815\n",
            "Epoch 8/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9561 - loss: 0.1774 - val_accuracy: 0.9550 - val_loss: 0.1830\n",
            "Epoch 9/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9562 - loss: 0.1773 - val_accuracy: 0.9553 - val_loss: 0.1814\n",
            "Epoch 10/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1752 - val_accuracy: 0.9554 - val_loss: 0.1810\n"
          ]
        }
      ],
      "source": [
        "# Define the MLP Model\n",
        "mlp_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_fraud_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "mlp_history = mlp_model.fit(X_train_fraud_scaled, y_train_fraud,\n",
        "                            epochs=10, batch_size=32,\n",
        "                            validation_data=(X_test_fraud_scaled, y_test_fraud))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHhTC1lFIn5i"
      },
      "source": [
        "Convolutional Neural Network (CNN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H99I6n_DIxZ1"
      },
      "source": [
        "Since CNNs are built for spatial data, we reshape the data so that each feature becomes like a “pixel” in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syw5OAn1IpNn",
        "outputId": "7b631ccc-c97c-436b-cd63-999f2cc45959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9408 - loss: 0.2228 - val_accuracy: 0.9521 - val_loss: 0.1934\n",
            "Epoch 2/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9526 - loss: 0.1908 - val_accuracy: 0.9523 - val_loss: 0.1894\n",
            "Epoch 3/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9538 - loss: 0.1861 - val_accuracy: 0.9524 - val_loss: 0.1891\n",
            "Epoch 4/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9549 - loss: 0.1829 - val_accuracy: 0.9534 - val_loss: 0.1882\n",
            "Epoch 5/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5ms/step - accuracy: 0.9559 - loss: 0.1796 - val_accuracy: 0.9537 - val_loss: 0.1857\n",
            "Epoch 6/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9554 - loss: 0.1807 - val_accuracy: 0.9538 - val_loss: 0.1851\n",
            "Epoch 7/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1761 - val_accuracy: 0.9538 - val_loss: 0.1855\n",
            "Epoch 8/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - accuracy: 0.9557 - loss: 0.1794 - val_accuracy: 0.9544 - val_loss: 0.1844\n",
            "Epoch 9/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - accuracy: 0.9548 - loss: 0.1825 - val_accuracy: 0.9542 - val_loss: 0.1841\n",
            "Epoch 10/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.9550 - loss: 0.1818 - val_accuracy: 0.9541 - val_loss: 0.1842\n"
          ]
        }
      ],
      "source": [
        "# Reshape for CNN: (samples, features, 1)\n",
        "X_train_fr_cnn = X_train_fraud_scaled.reshape(X_train_fraud_scaled.shape[0], X_train_fraud_scaled.shape[1], 1)\n",
        "X_test_fr_cnn = X_test_fraud_scaled.reshape(X_test_fraud_scaled.shape[0], X_test_fraud_scaled.shape[1], 1)\n",
        "\n",
        "# Define the CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_fraud_scaled.shape[1], 1)),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "cnn_history = cnn_model.fit(X_train_fr_cnn, y_train_fraud,\n",
        "                            epochs=10, batch_size=32,\n",
        "                            validation_data=(X_test_fr_cnn,  y_test_fraud))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s8d9n7aKDBN"
      },
      "source": [
        "Recurrent Neural Network (RNN)\n",
        "\n",
        "RNNs are used for sequential data. We use the same reshaped data as for the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GYY9eSOKK_S",
        "outputId": "8a8ffcb5-b511-4aef-d24e-b757bef6e78b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9342 - loss: 0.2380 - val_accuracy: 0.9524 - val_loss: 0.1899\n",
            "Epoch 2/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - accuracy: 0.9547 - loss: 0.1843 - val_accuracy: 0.9533 - val_loss: 0.1864\n",
            "Epoch 3/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.9545 - loss: 0.1841 - val_accuracy: 0.9525 - val_loss: 0.1875\n",
            "Epoch 4/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6ms/step - accuracy: 0.9570 - loss: 0.1762 - val_accuracy: 0.9545 - val_loss: 0.1838\n",
            "Epoch 5/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - accuracy: 0.9568 - loss: 0.1769 - val_accuracy: 0.9539 - val_loss: 0.1845\n",
            "Epoch 6/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 7ms/step - accuracy: 0.9560 - loss: 0.1789 - val_accuracy: 0.9549 - val_loss: 0.1827\n",
            "Epoch 7/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.9560 - loss: 0.1794 - val_accuracy: 0.9551 - val_loss: 0.1816\n",
            "Epoch 8/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.9560 - loss: 0.1788 - val_accuracy: 0.9549 - val_loss: 0.1819\n",
            "Epoch 9/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - accuracy: 0.9570 - loss: 0.1758 - val_accuracy: 0.9551 - val_loss: 0.1822\n",
            "Epoch 10/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.9565 - loss: 0.1769 - val_accuracy: 0.9543 - val_loss: 0.1863\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    SimpleRNN(32, activation='relu', input_shape=(X_train_fraud_scaled.shape[1], 1)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "rnn_history = rnn_model.fit(X_train_fr_cnn, y_train_fraud,\n",
        "                            epochs=10, batch_size=32,\n",
        "                            validation_data=(X_test_fr_cnn, y_test_fraud))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvwBRuyiNDF8"
      },
      "source": [
        "Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM networks are a special kind of RNN capable of learning long-term dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTWx47PvNIs1",
        "outputId": "891d4568-6d30-41d4-9d29-387c19a9409d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 24ms/step - accuracy: 0.9312 - loss: 0.2396 - val_accuracy: 0.9508 - val_loss: 0.1932\n",
            "Epoch 2/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9549 - loss: 0.1821 - val_accuracy: 0.9541 - val_loss: 0.1848\n",
            "Epoch 3/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23ms/step - accuracy: 0.9566 - loss: 0.1775 - val_accuracy: 0.9542 - val_loss: 0.1850\n",
            "Epoch 4/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 24ms/step - accuracy: 0.9559 - loss: 0.1792 - val_accuracy: 0.9546 - val_loss: 0.1826\n",
            "Epoch 5/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 25ms/step - accuracy: 0.9560 - loss: 0.1789 - val_accuracy: 0.9550 - val_loss: 0.1824\n",
            "Epoch 6/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9567 - loss: 0.1764 - val_accuracy: 0.9551 - val_loss: 0.1814\n",
            "Epoch 7/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 22ms/step - accuracy: 0.9567 - loss: 0.1768 - val_accuracy: 0.9550 - val_loss: 0.1821\n",
            "Epoch 8/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 22ms/step - accuracy: 0.9565 - loss: 0.1770 - val_accuracy: 0.9550 - val_loss: 0.1818\n",
            "Epoch 9/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 23ms/step - accuracy: 0.9560 - loss: 0.1784 - val_accuracy: 0.9551 - val_loss: 0.1822\n",
            "Epoch 10/10\n",
            "\u001b[1m3778/3778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9566 - loss: 0.1766 - val_accuracy: 0.9550 - val_loss: 0.1824\n"
          ]
        }
      ],
      "source": [
        "lstm_model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(X_train_fraud_scaled.shape[1], 1)),\n",
        "    LSTM(50),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "lstm_history = lstm_model.fit(X_train_fr_cnn, y_train_fraud,\n",
        "                              epochs=10, batch_size=32,\n",
        "                              validation_data=(X_test_fr_cnn, y_test_fraud))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3jHEs8FW_Yy"
      },
      "source": [
        "MLOps – Experiment Tracking with MLflow\n",
        "MLflow lets us log parameters, metrics, and even the trained model so that you can track your experiments over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzztfuxGXCxr",
        "outputId": "04c6f4d6-3986-4c74-ab22-891f35d1ede3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[31m2025/02/11 13:02:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged Random Forest Model with accuracy: 0.21271879032524899\n"
          ]
        }
      ],
      "source": [
        "# Install MLflow if you haven't already (uncomment the next line if needed)\n",
        "# !pip install mlflow\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "mlflow.set_experiment(\"Fraud Detection Experiment\")\n",
        "# Predict using Random Forest model:\n",
        "y_pred_rf = rf_model.predict(X_test_fraud_scaled)  # Get predictions for Random Forest\n",
        "\n",
        "# MLflow Experiment Tracking\n",
        "mlflow.set_experiment(\"Fraud Detection Experiment\")\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_param(\"model\", \"RandomForest\")\n",
        "    accuracy_rf = accuracy_score(y_test_fraud, y_pred_rf)  # Use y_pred_rf here\n",
        "    mlflow.log_metric(\"accuracy\", accuracy_rf)\n",
        "    mlflow.sklearn.log_model(rf_model, \"random_forest_model\")\n",
        "    print(\"Logged Random Forest Model with accuracy:\", accuracy_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Jk35nQRVaQpj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "y_pred_lr = log_reg.predict(X_test_fraud_scaled)\n",
        "\n",
        "# Store model performance\n",
        "experiment_results = {\n",
        "    \"Logistic Regression\": accuracy_score(y_test_fraud , y_pred_lr),\n",
        "    \"Decision Tree\": accuracy_score(y_test_fraud , y_pred_rf ),\n",
        "    \"Random Forest\": accuracy_score(y_test_fraud,  y_pred_rf),\n",
        "    \"XGBoost\": accuracy_score(y_test_fraud , y_pred_xgb),\n",
        "}\n",
        "# Save JSON\n",
        "with open(\"experiment_results.json\", \"w\") as f:\n",
        "    json.dump(experiment_results, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU7fUeYaYKiX"
      },
      "source": [
        "Saving the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akPXiTDrYQoz",
        "outputId": "b4faf23b-d38d-42b3-9c04-6d47bd19b04c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fraud_random_forest_model.pkl']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Assume rf_model is your trained Random Forest model\n",
        "joblib.dump(rf_model, 'fraud_random_forest_model.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlSgGlkjZrCM"
      },
      "source": [
        "Save & Download Processed Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMBCbBUebmuy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvwaw9JFbq2S"
      },
      "source": [
        " Save & Download ML Models (Sklearn-based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnMgTOmzaB-n",
        "outputId": "21bd4437-97e2-4859-a4c2-4cc47dd6a084"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fraud_xgboost.pkl']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Save Logistic Regression model\n",
        "joblib.dump(log_reg, \"fraud_logistic_regression.pkl\")\n",
        "joblib.dump(dt_model, \"fraud_decision_tree.pkl\")\n",
        "joblib.dump(rf_model, \"fraud_random_forest.pkl\")\n",
        "joblib.dump(xgb_model, \"fraud_xgboost.pkl\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQyT8PYaHOB"
      },
      "source": [
        "Save & Download Deep Learning Models (TensorFlow/Keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHaSZG8raIJ-",
        "outputId": "1ad90e18-dbef-4607-f28e-3f5d4d1a05d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save MLP Model\n",
        "mlp_model.save(\"fraud_mlp_model.h5\")\n",
        "\n",
        "# Save CNN Model\n",
        "cnn_model.save(\"fraud_cnn_model.h5\")\n",
        "\n",
        "# Save RNN Model\n",
        "rnn_model.save(\"fraud_rnn_model.h5\")\n",
        "\n",
        "\n",
        "# Save LSTM Model\n",
        "lstm_model.save(\"fraud_lstm_model.h5\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
